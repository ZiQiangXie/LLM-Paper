# LLM-Paper
看July大神的博客感触很大，决定也认真读一下LLM相关的论文，并记录下学习过程。

道阻且长，行则将至。希望自己能坚持下去。

大神链接：https://blog.csdn.net/v_JULY_v/article/details/129508065



- [x] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models 





论文概览

1 GPT

Improving Language Understanding by Generative Pre-Training

https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf

2 GPT2

Language Models are Unsupervised Multitask Learners

https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf

3 GPT3

Language Models are Few-Shot Learners

https://arxiv.org/pdf/2005.14165.pdf

4 Instruct GPT

Training language models to follow instructions with human feedback

https://arxiv.org/pdf/2203.02155.pdf

5 T5

 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

https://arxiv.org/pdf/1910.10683.pdf

6 LaMDA

LaMDA: Language Models for Dialog Applications

https://arxiv.org/pdf/2201.08239.pdf

7 FLAN

Finetuned Language Models Are Zero-Shot Learners

https://arxiv.org/pdf/2109.01652.pdf

8 Pathways

PaLM: Scaling Language Modeling with Pathways

https://arxiv.org/pdf/2204.02311.pdf

9 Claude

Constitutional AI: Harmlessness from AI Feedback

https://arxiv.org/pdf/2212.08073.pdf

10 Sparrow

Improving alignment of dialogue agents via targeted human judgements

https://arxiv.org/pdf/2209.14375.pdf

11 GPT-4 技术报告

 GPT-4 Technical Report

https://arxiv.org/pdf/2303.08774.pdf













































