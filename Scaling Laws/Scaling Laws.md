##### Scaling Laws for Neural Language Models



关于$C\approx 6ND$的解释。

$C$ 表示算力的大小（Tflops），不包括Embedding；

$N$ 表示模型的参数量，不包括Embedding；

$D$ 表示模型训练的Token数量；

6的来源：

1）forward过程，主要是矩阵乘法，矩阵乘包括一次乘和一次加，每个参数对应2次浮点计算；

2）backward过程，计算每个参数的梯度和更新weights，计算量是forward的2倍，每个参数有4次浮点运算；

因此在模型训练过程中，每个参数要计算6次浮点运算；

备注：大模型一般只训练1个epoch。emm......



反Scaling Law

假设遵循计算效率最优来研发 LLM，那么根据 Scaling Law，给定模型大小，可以推算出最优的计算量，进一步根据最优计算量就能推算出需要的 token 数量，然后训练就行。

但是计算效率最优这个观点是针对训练阶段而言的，并不是推理阶段，实际应用中推理阶段效率更实用。

Meta 在 LLaMA 的观点是：给定模型的目标性能，并不需要用最优的计算效率在最快时间训练好模型，而应该在更大规模的数据上，训练一个相对更小模型，这样的模型在推理阶段的成本更低，尽管训练阶段的效率不是最优的（同样的算力其实能获得更优的模型，但是模型尺寸也会更大）。根据 Scaling Law，10B 模型只需要 200B 的数据，但是作者发现 7B 的模型性能在 1T 的数据后还能继续提升。

【参考】https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/134977341



















