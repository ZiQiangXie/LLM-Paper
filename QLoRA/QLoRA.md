##### QLoRA



1. 尽管 LLM 训练（或者一般在 GPU 上训练模型时）具有固有的随机性，但多次运行的结果仍然非常一致。
2. 如果您受到 GPU 内存的限制，QLoRA 提供的权衡可能是值得的。它节省了33% 的内存，但运行时间增加了39%。

QLoRA 是由 Tim Dettmers 等人提出的量化 LoRA 的缩写。QLoRA 是一种在微调过程中进一步减少内存占用的技术。在反向传播过程中，QLoRA 将预训练的权重量化为 4-bit，并使用分页优化器来处理内存峰值。

使用LoRA时可以节省33%的GPU内存。然而，由于QLoRA中预训练模型权重的额外量化和去量化，训练时间增加了39%。

默认LoRA具有16bit浮点精度：

训练时长：1.85 小时
内存占用：21.33GB
具有4位正常浮点数的 QLoRA

训练时长为：2.79h
内存占用为：14.18GB



QLoRA创新点：

基础知识：

量化是将高精度数值用低精度数值表示的方法；如fp32->int8，将源数据每个值除以一个绝对值最大值、乘以目标数据的最大值，因此存在一个高精度缩放系数，用于进行反量化；

由于存在异常值，会影响量化精度，所以量化一般都是分块进行的。将数据分成N份，分别对每份进行量化，这样每份数据就有一个量化系数。

1）NF4量化，节省显存、精度损失小；

就是所谓的分位数量化，信息论最优。

分位数量化（Quantile Quantization）是隶属于非线性量化。. 分位数 （Quantile）在数学上的定义指的是把 顺序排列 的一组数据分割为若干个相等块的分割 …

由于异常值的存在会导致所在分块的其他值容易出现量化误差，使得大部分数都集中在很小的区域里。因此比较”理想”的情况是4bit（或者8bit）的每一个”数”都是相同的概率被用到的。比如对于符合某个分布（比如正态分布）的数据的N（比如N=1600）个采样，16个数（索引）都被量化过N/16=100次。

一般情况下低精度数据类型能表示的数比较少，如果由于异常值的存在导致量化后分布不均匀，比如集中在某个小区域内，那么本来就少的数实际用到的就更少，也就会带来更大的误差。所以最好的情况就是低精度能表示的数都被均匀的用到，这样才能充分利用，更加合理的表示高精度数值。

在量化时，先计算高精度数的概率分布，然后将概率分布函数划分为$2^k$个区间，k表示4、8bit，其中每个区间有相等的面积，这样就可以将落到同一个区间的值用同一个量化后的值表示，使得所有的数值分布均匀。



2）Double Quantizition，双量化，量化量化参数，进一步节省显存；

量化是将高精度数值用低精度数值表示的方法；如fp32->int8，将源数据每个值除以一个绝对值最大值、乘以目标数据的最大值，因此存在一个高精度缩放系数，用于进行反量化；

由于存在异常值，会影响量化精度，所以量化一般都是分块进行的。将数据分成N份，分别对每份进行量化，这样每份数据就有一个量化系数。

假设第一次量化时每份数据包含64个参数，需要一个fp32的量化系数，那么平均每个参数多占用为32/64=0.5bit。

第二次量化，量化的是上一步生成的fp32的量化系数，由于每块数据仅有一个系数，因此第二次量化实际上包含了很多块原始数据，需要注意理解。

假设第二次量化时每份数据为256个参数，且将每个参数量化为FP8，每块需要一个fp32的量化系数。因此双量化就是把fp32的量化系数再量化成FP8，同时再生成一个fp32的量化系数。这时每个参数增加的占用就是8/64+32/(64*256)。

8/64：表示第一次量化时，每64个数据对应的量化系数，在第二次量化时把它量化成了FP8；

32/(64*256)：32表示第二次量化时每块数据的量化系数，但是第二次量化的这个系数对应了原始参数的数量变成了64\*256个。因为第一次是64个数据只生成一个量化系数，而第二次量化是每256个数一起量化，才生成一个量化系数，这256个数的每一个都对应了第一次量化的64个参数，所以第二次量化的系数对应了64\*256个原始参数。

**注意不是每个权重值量化所需要的空间，而是所需要的额外空间。**

3）Paged Optimizers，内存统一管理，避免activation checkpoint时的内存峰值爆显存；

其实就是快爆时卸载到CPU一部分暂时不用的优化器参数；

这种技术利用了NVIDIA统一内存的特性，实现了CPU和GPU之间自动的页面转换。当GPU内存不足时，Paged Optimizers技术会自动将优化器状态转移到CPU内存，以确保优化器的正常运行。
Paged Optimizers：分页优化器，是一种能够在CPU和GPU之间自动转换优化器状态的技术。
NVIDIA统一内存：一种将CPU和GPU内存统一管理的技术，可以让CPU和GPU共享同一块内存，从而减少数据传输的时间和开销。

4）**增加Adapter**：4-bit的NormalFloat与Double Quantization，节省了很多空间，但带来了性能损失，作者通过插入更多adapter来弥补这种性能损失。在LoRA中，一般会选择在query和value的全连接层处插入adapter。而QLoRA则在所有全连接层处都插入了adapter，增加了训练参数，弥补精度带来的性能损失。





QLoRA+more adapter，由于Lora效果，因此微调更多的参数对提升精度的影响较大，r的值影响较小；

QLoRA精度几乎无损，达到LoRA甚至full fp16的精度；

1）QLoRA是量化后微调，相较于微调后量化精度损失更小；

2）量化节省显存，可以扩大adapter数量，微调更多的参数，提升指标；

3）NF4是信息论最优方法，量化精度本身就比int4高；



NF4、FP4、FP8、int8等数据类型

INT8：将浮点数转换为8位整数，可以大幅度减少模型的大小和计算量，但需要注意的是，INT8量化可能会导致模型精度损失，需要合理的量化策略和校准方法来平衡性能和精度。

FP4：使用4位浮点数表示数值，相比于标准的32位浮点数，FP4可以显著减少模型的大小和计算量，同时保持较好的模型精度。

NF4是一种基于归一化浮点数的量化方法，通过将浮点数归一化到[-1,1]区间，并使用4位数值表示，可以在保证模型精度的同时，实现高效的模型优化。

https://developer.nvidia.com/zh-cn/blog/fp8-precision-performance/

https://blog.csdn.net/qq_29788741/article/details/134472717

https://cloud.tencent.com/developer/article/2335311





【参考】https://cloud.tencent.com/developer/article/2375230



https://blog.csdn.net/HERODING23/article/details/131584089

https://blog.csdn.net/qq_16949707/article/details/131024256

NF4量化详解

https://zhuanlan.zhihu.com/p/666234324

https://zhuanlan.zhihu.com/p/654967425

思维导图

https://zhuanlan.zhihu.com/p/635345199

分位数量化的详解

http://fancyerii.github.io/2023/12/14/qlora/



https://www.cnblogs.com/huggingface/p/17816374.html





校准数据在量化过程中既对权重的量化起作用，也对激活的量化起作用。量化过程通常包括对模型中的权重和激活值进行量化，而校准数据用于确定这两个方面的量化参数。

对权重量化的作用：
权重校准：在权重量化中，校准数据用于分析模型权重的分布情况。通过观察在这些数据上权重的激活模式，可以确定最佳的量化比例因子和零点，以便在量化后保持权重的重要性和效果。

最小化误差：通过使用校准数据，可以最小化量化引入的误差。权重的量化参数被选定以确保量化后的权重能够尽可能地代表原始的浮点权重，从而减少对模型性能的影响。

对激活量化的作用：
激活校准：在激活量化中，校准数据用于捕捉模型输出的分布。通过分析这些数据通过模型产生的激活值，可以确定合适的量化参数，以确保量化后的激活值能够准确反映模型的输出。

动态范围覆盖：激活的量化参数需要能够覆盖模型在实际使用中可能遇到的所有激活值的动态范围。校准数据集应该包含足够多样化的样本，以确保量化参数能够适应各种可能的输入情况。

权重和激活的联合校准：
在实际的量化过程中，权重和激活的量化通常是相互关联的。权重的量化会影响激活的分布，反之亦然。因此，校准数据不仅用于独立地确定权重和激活的量化参数，还用于确保整个量化模型的行为与未量化模型尽可能一致。

为了实现这一点，通常会采用以下方法：

端到端校准：使用整个模型和校准数据集进行端到端的量化参数确定。这种方法同时考虑了权重和激活的量化，以确保整个模型的输出尽可能接近原始模型。

迭代校准：在一些情况下，可能需要多次迭代来优化权重和激活的量化参数。每次迭代都会使用校准数据来调整参数，直到达到满意的精度和效率。

总之，校准数据在量化过程中确保了权重和激活的量化参数能够准确反映模型的行为，从而在减少模型大小和计算需求的同时，尽量保持模型的性能。













