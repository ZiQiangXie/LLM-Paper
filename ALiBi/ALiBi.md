##### ALiBi

Train Short Test Long Attention with Linear Biases Enables Input Length Extrapolation

#### 介绍

ALiBi（Attention with Linear Biases）方法是一种新颖的位置编码方法，它旨在改进Transformer模型处理长序列时的性能。这篇论文的核心观点可以总结如下：

* 问题陈述：变换器模型（如Transformer）在训练时使用的序列长度（L）通常与推理时的序列长度相同。然而，模型在推理时处理比训练时更长序列的能力（即外推能力）是一个未解决的问题。

* 现有方法的局限性：作者首先指出，现有的基于正弦位置嵌入的方法在处理比训练时更长的序列时表现不佳，外推能力有限。

* ALiBi方法：为了解决这一问题，作者提出了一种新的方法——带有线性偏差的注意力（ALiBi）。这种方法通过在查询-键注意力得分上施加与距离成比例的线性惩罚来引入位置信息，而不是添加位置嵌入。

* 效率提升：ALiBi方法能够在不增加额外运行时或参数的情况下，使用更短的训练序列，同时在运行时可靠地应用于更长的序列。这使得模型在训练时更加高效，同时在推理时能够处理更长的输入。

* 实验结果：通过在WikiText-103和Toronto BookCorpus等数据集上的实验，作者证明了ALiBi方法在不外推时能够达到或超过正弦位置嵌入方法的性能，并且在外推时能够维持强大的性能。

* 泛化能力：ALiBi方法展示了良好的泛化能力，即使在不同的文本域和模型大小下，使用相同的斜率集（slopes）也能得到一致的结果，无需针对每个新数据集进行超参数调整。

* 未来方向：作者提出，尽管ALiBi在处理更长序列时能够减少“早期标记诅咒”（early token curse），但未来工作可以通过更有效地利用更长的历史信息来进一步提高性能。

总的来说，论文的核心贡献是提出了一种简单而有效的方法（ALiBi），使得变换器模型能够在推理时处理比训练时更长的序列，这对于提高模型的灵活性和效率具有重要意义。

#### ALiBi：

##### 原理

相对位置编码：ALiBi是一种相对位置编码方法，它不像传统的正弦位置编码那样将位置信息直接添加到词嵌入中，而是在注意力机制中对键（key）和查询（query）之间的注意力得分施加线性偏差。

线性偏差：ALiBi方法在计算注意力得分时，会根据键和查询之间的距离引入一个线性偏差。这种偏差随着键和查询之间距离的增加而线性增加，从而鼓励模型关注更接近的词。

归纳偏差：ALiBi引入了一种归纳偏差，倾向于近期性，即模型更关注序列中较近的词，这有助于在处理长序列时维持性能。

##### 实现

注意力得分计算：在标准的Transformer模型中，注意力得分是通过计算查询和键的点积，然后应用softmax函数来获得的。ALiBi方法在点积之后，但在应用softmax之前，会向注意力得分添加一个静态的、非学习的线性偏差。

线性偏差的添加：线性偏差是根据键和查询之间的距离计算的，具体来说，对于每个头（head），都有一个特定的斜率（slope），这个斜率是固定的，并且在训练前就设定好了。

几何序列的斜率：ALiBi方法使用几何序列来设置不同头的斜率。例如，对于8个头的模型，斜率可以是1/2的幂次方，而对于16个头的模型，则是在1/√2和1之间几何平均得到。

##### 作用

提高外推能力：ALiBi方法使得模型能够在训练时使用较短的序列，而在推理时处理更长的序列，从而提高了模型的外推能力。

减少资源消耗：由于ALiBi不需要额外的运行时操作，并且在训练时可以使用更短的序列，因此它能够减少内存使用和加快训练速度。

提升性能：在多个基准测试中，ALiBi方法在不进行外推时能够达到或超过正弦位置嵌入方法的性能，并且在外推时能够维持强大的性能。

总结来说，ALiBi方法通过在注意力机制中引入与距离成比例的线性偏差，有效地提高了Transformer模型处理长序列的能力，同时保持了模型的效率和灵活性。这种方法的简单性和有效性使其成为一个有前景的研究方向，对于自然语言处理领域中的长序列建模具有重要意义。

#### 对比

在论文中，作者提到了几种其他的位置编码方法，并与ALiBi方法进行了对比。以下是这些方法的简要介绍以及与ALiBi的对比分析：

* 正弦位置编码（Sinusoidal Positional Embeddings）:

这是Transformer模型中最初使用的位置编码方法。
位置信息通过固定的正弦和余弦函数生成的嵌入向量来编码，并添加到词嵌入中。
ALiBi对比：ALiBi不使用额外的位置嵌入，而是通过在注意力机制中引入线性偏差来编码位置信息。这种方法使得ALiBi在处理长序列时能够更有效地进行外推，且不需要额外的内存消耗。

* 可学习的位置嵌入（Learnable Positional Embeddings）:

这种方法通过训练过程学习位置嵌入，使得模型能够适应不同的数据集和任务。
ALiBi对比：ALiBi方法的一个优势是不需要学习位置嵌入，这减少了模型的参数数量，简化了训练过程，并可能提高了模型的泛化能力。

* 旋转位置编码（Rotary Positional Embeddings）:

这种方法通过将位置信息编码到注意力机制的键和查询中，而不是在词嵌入中。
ALiBi对比：ALiBi与之类似，也是在注意力机制中编码位置信息，但ALiBi使用的是简单的线性偏差，而不是旋转编码。这使得ALiBi在实现上更简洁，且据论文称，ALiBi在WikiText-103基准测试中表现更好。

* T5模型的相对位置编码（T5 Bias）:

T5模型使用一种相对位置方法，通过在注意力得分上添加依赖于查询和键之间距离的偏差来编码位置信息。
ALiBi对比：尽管T5 Bias方法允许一定程度的外推，但它在计算上比较昂贵，训练速度较慢。ALiBi通过使用固定的线性偏差，实现了更快的训练速度和更低的内存使用，同时在多个数据集上保持了竞争力的性能。

* Transformer-XL:

Transformer-XL通过使用缓存机制来处理长序列，并在推理时能够处理比训练时更长的序列。
ALiBi对比：ALiBi不需要复杂的缓存机制，它通过简单的线性偏差来实现外推，这使得ALiBi在实现上更为直接和高效。
总结来说，ALiBi的主要优势在于其简单性、高效性和无需额外参数。它通过在注意力机制中引入线性偏差来编码位置信息，这使得模型能够在不增加额外计算成本的情况下处理更长的序列，并且在多个基准测试中显示出良好的性能。此外，ALiBi的实现简单，易于集成到现有的Transformer模型中，这为未来的研究和应用提供了便利。