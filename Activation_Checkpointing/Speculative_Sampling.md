##### Activation checkpointing Training Deep Nets with Sublinear Memory Cost 

### 导读

在深度学习模型的训练过程中，前向传播会计算并存储每一层的激活值，这些激活值在后向传播时被用来计算梯度。然而对于深度很大的模型，这种方式可能会导致内存溢出，因为需要存储大量的激活值。如今（2023年）大模型时代来临，参数量巨大使得即使将batch_size设置为1并使用梯度累积的方式更新，也仍然会OOM。原因就是需要将所有前向传播时的激活值保存下来，消耗大量显存。还有另外一种延迟计算的思路，丢掉前向传播时的激活值，在计算梯度时需要哪部分的激活值就重新计算哪部分的激活值，这样做倒是解决了显存不足的问题，但加大了计算量同时也严重拖慢了训练。

激活检查点 （Activation Checkpointing）也称梯度检查点（Gradient Checkpointing） ，是一种通过清除前向传播中某些层的激活，并在向后传播过程中重新计算的方式，实现减少内存使用的技术。在前向传播过程中只存储一部分（而不是全部）的激活值，通过控制保存的比例可以在内存使用和计算效率之间进行权衡，在上述两种方式之间取了一个平衡。这种方法选择计算图上的一部分激活值保存下来，其余部分丢弃。对于没有存储的激活值，如果在后向传播过程中需要它们，就重新计算这些值。

 实际上，这会用额外的计算时间来减少内存使用量。 如果模块处于检查点，则在前向传递结束时，该模块的输入和输出将保留在内存中。 在前向传递过程中，任何本来可以参与该模块内部计算的中间张量都会被释放。 在向后传递检查点模块时，会重新计算这些张量。 此时，该检查点模块之外的各层已经完成了向后传递，因此检查点的峰值内存使用量可能会更低。

优点：可以在有限的硬件资源下训练更大的模型；

不足：会增加一些计算开销导致训练变慢，因为需要重新计算一些激活值。

因此这种技术在训练大型模型（特别是在内存有限的设备上）时非常有用，先解决放得下的问题，再解决训练慢的问题。例如，微软的深度速度优化库 DeepSpeed 就使用了激活检查点技术来实现在单个GPU上训练数十亿参数的模型。






